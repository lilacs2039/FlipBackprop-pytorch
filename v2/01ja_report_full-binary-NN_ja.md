# 反転逆伝搬法とFBF学習によるFull binary NNの学習

# サマリー



前レポートでは直列逆伝播による学習を提案したが、重みの優先的な学習が更新信号消失の原因となっていた。そこで、入力層を優先して学習し、２回目のForwardでバイナリ重みを更新するFBF(Forward-Backward-Forward)学習を導入することで、勾配消失を改善することが可能となった。

本手法で、重みがすべてバイナリ値であるバイナリニューラルネットワーク（Full binary NN）を学習することにより、irisデータセットで90%程度の精度を達成した。



本記事を読む前提：「[Flip backprop](../v1/01_report_flip-backprop.md)」について知っていること。



# FBF学習

## 課題：重み優先 直列逆伝播は更新信号消失が発生する

前記事では、直列逆伝播は重みを優先して学習していた。

**図．重み優先 直列逆伝播の実行順序**

![image-20230606104402698](01ja_report_full-binary-NN_ja.assets/image-20230606104402698.png)



しかし、学習初期の時点で、前方レイヤ（入力側）への逆伝播のflipがほとんど0であり（更新信号消失）、学習が進まないことに気づいた。FP32学習の勾配消失のような現象で、更新信号が小さいため前方レイヤの学習が進まない状況である。

原因は、直列逆伝播の順序にあると考えた。重み優先の直列逆伝播では、後方レイヤの重みを先に更新してから、辻褄が合うように前方レイヤの更新信号（反転）を計算する。この場合、先に学習された後方レイヤが更新信号を小さくしてしまうので、前方レイヤの学習が進みにくい。

では逆に、前方レイヤを先に更新して、更新された入力と辻褄が合うように後方レイヤの重みを更新すればどうか。前方レイヤは学習が進み、後方レイヤがさらに誤差を小さくするよう学習されるのではないだろうか。



## 解決策：優先逆伝播とFBF学習

そこで、前方レイヤ（入力側）を優先的に学習する入力優先の直列逆伝播と、それを実現するFBF（Forward-Backward-Forward）学習を考えた。

FBF学習の動作は、Backward時に入力側を優先して学習し、２回目のForwardで前方レイヤから順番にバイナリ重みを更新する。こうすることで、前方レイヤの更新に辻褄が合うように後方レイヤを学習できる。

**図．入力優先 直列逆伝播の実行順序**

![image-20230606165625492](01ja_report_full-binary-NN_ja.assets/image-20230606165625492.png)

１ステップの学習が1.5往復に増加することはデメリットであるが、１回の更新の正確さは向上すると考えられる。１回の更新信号の正確さが重要なバイナリ学習に適した手法と考えられる。



























